# -*- coding: utf-8 -*-
"""LSTM_regression_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZKefWn3FpoeezvV2hjTZVj3SeEiUtxFw
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn
from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split
import time as time
import re
from random import sample

from google.colab import drive
drive.mount('/content/drive')

train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/yelp_train_real.csv')
validate = pd.read_csv('/content/drive/My Drive/Colab Notebooks/yelp_val_real.csv')

import nltk
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 
import re
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

def stem_phrase(phrase):
    words = phrase.split(" ")
    stemmed_words = []
    for word in words:
        stemmed_word = stemmer.stem(word)
        stemmed_words.append(stemmed_word)
    stemmed_phrase = " ".join(stemmed_words)
    return stemmed_phrase

# you can add if you find some additional fix or cleaning
def clean_text(phrase):
    phrase = phrase.replace("n't", " not")
    phrase = phrase.replace("it's", "it is")
    phrase = phrase.replace("'v", " have")
    return phrase

def cleaning(data):
    data.dropna()
    tqdm.pandas(desc="Stemming...")
    data['text'] = data['text'].progress_apply(stem_phrase)
    data['text'] = data['text'].progress_apply(clean_text)
    return data

def cln(data):
    cln_text =[]
    for sentence in data["text"]:
        cln_text.append(re.sub("[!&\'()*+,-./:;<=>?@[\\]^_`{|}~]", "", sentence))
    data["no_punc_text"] = cln_text
    return data

def rm_stopwords(data):
    data_cl = cln(data)
    filtered_sentence=[]
    for sentence in data_cl["no_punc_text"]:
        word_tokens = word_tokenize(sentence) 
        sentence_tem = [w for w in word_tokens if not w in stop_words]
        filtered_sentence.append(sentence_tem)
    data["cln_text"] = filtered_sentence
    clc_sentence = []
    for sentence in data["cln_text"]:
        clc_sentence.append(" ".join(sentence))
    data["cln_text"]=clc_sentence
    return data

train = rm_stopwords(train)
validate = rm_stopwords(validate)

# Hyperparameters
batch_size = 32
embed_size = 100
hidden_size = 64

# Split review text in to lists of words
import re
split_train_text=[]
one_star = train[train['stars']==1].sample(2500)
two_star = train[train['stars']==2].sample(2500)
three_star = train[train['stars']==3].sample(2500)
four_star = train[train['stars']==4].sample(2500)
five_star = train[train['stars']==5].sample(2500)

for str in train['cln_text']:
    split_train_text.append(re.findall(r"[\w']+|[.,!?:;()]", str.lower()))
# for str in one_star['cln_text']:
#     split_train_text.append(re.findall(r"[\w']+|[.,!?:;()]", str.lower()))
# for str in two_star['cln_text']:
#     split_train_text.append(re.findall(r"[\w']+|[.,!?:;()]", str.lower()))
# for str in three_star['cln_text']:
#     split_train_text.append(re.findall(r"[\w']+|[.,!?:;()]", str.lower()))
# for str in four_star['cln_text']:
#     split_train_text.append(re.findall(r"[\w']+|[.,!?:;()]", str.lower()))
# for str in five_star['cln_text']:
#     split_train_text.append(re.findall(r"[\w']+|[.,!?:;()]", str.lower()))

split_val_text=[]
for str in validate['cln_text']:
    split_val_text.append(re.findall(r"[\w']+|[.,!?:;()]", str.lower()))

# Generate top n most frequent words from training data
all_words={}
for sentence in split_train_text:
    for word in sentence:
        if word in all_words:
            all_words[word] += 1
        else:
            all_words[word] = 1

n=5000
vocabulary=dict(zip(sorted(all_words,key=all_words.get,reverse=True)[:n],[i for i in range(n)]))

# Generate Training Data
text_train_data=[]
for sentence in split_train_text:
    train_sentence=[]
    for word in sentence:
        if word in vocabulary:
            train_sentence.append(vocabulary[word])
    text_train_data.append(train_sentence)

useful_train_data=[]
for useful in train['useful']:
    useful_train_data.append(useful)
# for useful in one_star['useful']:
#     useful_train_data.append(useful)
# for useful in two_star['useful']:
#     useful_train_data.append(useful)
# for useful in three_star['useful']:
#     useful_train_data.append(useful)
# for useful in four_star['useful']:
#     useful_train_data.append(useful)
# for useful in five_star['useful']:
#     useful_train_data.append(useful)

sentiment_train_data=[]
for sentiment in train['sentiment_score']:
    sentiment_train_data.append(sentiment)
# for sentiment in one_star['sentiment_score']:
#     sentiment_train_data.append(sentiment)
# for sentiment in two_star['sentiment_score']:
#     sentiment_train_data.append(sentiment)
# for sentiment in three_star['sentiment_score']:
#     sentiment_train_data.append(sentiment)
# for sentiment in four_star['sentiment_score']:
#     sentiment_train_data.append(sentiment)
# for sentiment in five_star['sentiment_score']:
#     sentiment_train_data.append(sentiment)

nchar_train_data=[]
for nchar in train['nchar']:
    nchar_train_data.append(nchar)
# for nchar in one_star['nchar']:
#     nchar_train_data.append(nchar)
# for nchar in two_star['nchar']:
#     nchar_train_data.append(nchar)
# for nchar in three_star['nchar']:
#     nchar_train_data.append(nchar)
# for nchar in four_star['nchar']:
#     nchar_train_data.append(nchar)
# for nchar in five_star['nchar']:
#     nchar_train_data.append(nchar)

nword_train_data=[]
for nword in train['nword']:
    nword_train_data.append(nword)
# for nword in one_star['nword']:
#     nword_train_data.append(nword)
# for nword in two_star['nword']:
#     nword_train_data.append(nword)
# for nword in three_star['nword']:
#     nword_train_data.append(nword)
# for nword in four_star['nword']:
#     nword_train_data.append(nword)
# for nword in five_star['nword']:
#     nword_train_data.append(nword)

train_data=[]
for i in range(len(split_train_text)):
    train_data.append([text_train_data[i],useful_train_data[i],sentiment_train_data[i],nchar_train_data[i],nword_train_data[i]])

# Generate Training Labels
train_labels=[]
for rating in train['stars']:
    train_labels.append(rating-1)
# for rating in one_star['stars']:
#     train_labels.append(rating-1)
# for rating in two_star['stars']:
#     train_labels.append(rating-1)
# for rating in three_star['stars']:
#     train_labels.append(rating-1)
# for rating in four_star['stars']:
#     train_labels.append(rating-1)
# for rating in five_star['stars']:
#     train_labels.append(rating-1)

# Generate Validation Data
val_text_data=[]
for sentence in split_val_text:
    val_sentence=[]
    for word in sentence:
        if word in vocabulary:
            val_sentence.append(vocabulary[word])
    val_text_data.append(val_sentence)

useful_val_data=[]
for useful in validate['useful']:
    useful_val_data.append(useful)

sentiment_val_data=[]
for sentiment in validate['sentiment_score']:
    sentiment_val_data.append(sentiment)

nchar_val_data=[]
for nchar in validate['nchar']:
    nchar_val_data.append(nchar)

nword_val_data=[]
for nword in validate['nword']:
    nword_val_data.append(nword)

val_data=[]
for i in range(len(validate)):
    val_data.append([val_text_data[i],useful_val_data[i],sentiment_val_data[i],nchar_val_data[i],nword_val_data[i]])

# Generate Validation Labels
val_labels=[]
for rating in validate['stars']:
    val_labels.append(rating-1)

class Dataset(Dataset):
    
    def __init__(self,data,labels):
        self.data = data
        self.labels = labels
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self,index):
        self.x_text = self.data[index][0]
        self.x_useful = self.data[index][1]
        self.x_sentiment = self.data[index][2]
        self.x_nchar = self.data[index][3]
        self.x_nword = self.data[index][4]
        self.x = [self.x_text,self.x_useful,self.x_sentiment,self.x_nchar,self.x_nword]
        self.y = self.labels[index]
        return self.x, self.y

def collate(seq_list):
    inputs,targets = zip(*seq_list)
    inputs = np.array(inputs)
    input_lens = torch.IntTensor([len(seq) for seq in inputs[:,0]])
    text = [torch.Tensor(inputs[:,0][i]) for i in range(len(inputs[:,0]))]
    text = rnn.pad_sequence(text,batch_first=True).type(torch.LongTensor)
    useful = torch.LongTensor([score for score in inputs[:,1]])
    sentiment = torch.LongTensor([sentiment for sentiment in inputs[:,2]])
    nchar = torch.LongTensor([num for num in inputs[:,3]])
    nword = torch.LongTensor([num for num in inputs[:,4]])
    targets = torch.FloatTensor([label for label in targets])
    final_inputs = [text,useful,sentiment,nchar,nword]
    return final_inputs,targets,input_lens

trainDataset = Dataset(train_data,train_labels)
valDataset = Dataset(val_data,val_labels)
train_loader = DataLoader(trainDataset, shuffle=True, batch_size=batch_size, collate_fn = collate)
val_loader = DataLoader(valDataset, shuffle=True, batch_size=batch_size, collate_fn = collate)

class Model(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super(Model, self).__init__()
        # Primary Classifier - RNN
        self.embedding = nn.Embedding(vocab_size,embed_size)
        self.drop1 = nn.Dropout(0.4)
        self.conv = nn.Conv1d(embed_size,embed_size,kernel_size=1,stride=1)
        self.drop2 = nn.Dropout(0.4)
        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True)
        #torch.nn.init.kaiming_normal_(self.rnn.weight_ih_l0, mode='fan_out', nonlinearity='leaky_relu')
        #torch.nn.init.kaiming_normal_(self.rnn.weight_hh_l0, mode='fan_out', nonlinearity='leaky_relu')
        torch.nn.init.xavier_normal_(self.rnn.weight_ih_l0, gain=1.0)
        torch.nn.init.xavier_normal_(self.rnn.weight_hh_l0, gain=1.0)
        self.rnn_scoring = nn.Linear(hidden_size*2,hidden_size)
        self.drop3 = nn.Dropout(0.4)
        self.scoring = nn.Linear(hidden_size,1)
        #self.tanh = nn.Tanh()

        # Secondary Classifier
        self.secondary_layers=[]
        self.secondary_layers.append(nn.Linear(4,hidden_size))
        self.secondary_layers.append(nn.Linear(hidden_size,64))
        self.secondary_layers.append(nn.Linear(64,1))
        self.secondary = nn.Sequential(*self.secondary_layers)

        self.secondary_linear = nn.Linear(4,hidden_size)

    
    def forward(self, inputs, text_lengths):

        # Split Input Data
        text = inputs[0]
        secondary = torch.stack([inputs[1],inputs[2],inputs[3],inputs[4]]).transpose(0,1).type(torch.FloatTensor).cuda()

        # Text Processing
        text = self.embedding(text).transpose(1,2)
        text = self.drop1(text)
        text = self.conv(text).transpose(1,2)
        text = self.drop2(text)
        packed_text = rnn.pack_padded_sequence(text, text_lengths, batch_first=True, enforce_sorted=False)
        out_text_packed,_ = self.rnn(packed_text)
        out_text,_ = rnn.pad_packed_sequence(out_text_packed)
        out_text = torch.mean(out_text,dim=0)
        secondary_out = self.secondary_linear(secondary)

        primary_pred = self.rnn_scoring(out_text)
        #pred = self.rnn_scoring(torch.cat([out_text,secondary_out],dim=1))
        pred = self.drop3(primary_pred)
        pred = self.scoring(primary_pred)

        # Secondary Feature Processing
        #secondary_pred = self.secondary(secondary)

        #Final Prediction
        out = pred
        #out = 2.5*(self.tanh(out)+1)
        return out
    
def init_weights(m):
    if type(m) == nn.Linear or type(m) == nn.Conv1d:
        torch.nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='leaky_relu')

# Training
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Model(len(all_words), embed_size, hidden_size)
#model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/yelp_rnn_v10.pt')) #
model.apply(init_weights)
model.to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)
train_loss=[]
val_loss=[]
train_accuracy=[]
val_accuracy=[]

for epoch in range(20):
    model.train()
    avg_train_loss = 0.0
    avg_val_loss = 0.0
    train_acc = 0.0
    batch_acc = 0.0
    val_acc = 0.0
    start = time.time()

    # Trains in Batches, gives statistics for each batch
    # Gives total training accuracy at the end (taken during training)
    for batch_idx,(inputs, targets, input_lens) in enumerate(train_loader):
        torch.cuda.empty_cache()
        
        inputs[0] = inputs[0].to(device)
        inputs[1] = inputs[1].to(device)
        inputs[2] = inputs[2].to(device)
        inputs[3] = inputs[3].to(device)
        inputs[4] = inputs[4].to(device)

        targets = targets.to(device)
        input_lens = input_lens.to(device)

        optimizer.zero_grad()
        out = model(inputs, input_lens)

        batch_acc = torch.sum(torch.round(out.reshape(-1)) == targets).item() #out was indices
        train_acc += batch_acc

        loss = criterion(out.reshape(-1),targets)

        avg_train_loss += loss.item()
        loss.backward()
        optimizer.step()
            
        torch.cuda.empty_cache()
        del inputs[0]
        del inputs[1]
        del inputs[2]
        del targets
        del input_lens
    
    end = time.time()
    #print('Saving model...')
    #model_save_name = 'yelp_rnn_v10.pt'
    #path = F"/content/drive/My Drive/Colab Notebooks/{model_save_name}" 
    #torch.save(model.state_dict(), path)
    print('Epoch ', epoch + 1, 'Train Loss ', avg_train_loss/len(train_loader))
    print('Train Acc:',train_acc/len(trainDataset))
    train_accuracy.append(train_acc/len(trainDataset))
    train_loss.append(avg_train_loss/len(train_loader))
    #print('Time ', end-start)
    
    # Calculates accuracy on Val Set
    model.eval()
    with torch.no_grad():
        for batch_idx,(inputs, targets, input_lens) in enumerate(val_loader):
            inputs[0] = inputs[0].to(device)
            inputs[1] = inputs[1].to(device)
            inputs[2] = inputs[2].to(device)
            inputs[3] = inputs[3].to(device)
            inputs[4] = inputs[4].to(device)

            targets = targets.to(device)
            input_lens = input_lens.to(device)

            optimizer.zero_grad()
            out = model(inputs, input_lens)
        
            values,indices = torch.max(out,dim=1)
            val_acc += torch.sum(torch.round(out.reshape(-1)) == targets).item()

            loss = criterion(out.reshape(-1), targets)
            avg_val_loss += loss.item()
        
            torch.cuda.empty_cache()
            del inputs[0]
            del inputs[1]
            del inputs[2]
            del targets
            del input_lens

    scheduler.step(val_acc)

    print('Epoch', epoch + 1, 'Val Loss', avg_val_loss/len(val_loader))
    print('Val Acc:',val_acc/len(valDataset))
    print('\n')
    val_accuracy.append(val_acc/len(valDataset))
    val_loss.append(avg_val_loss/len(val_loader))

#Plot Accuracy Curves
plt.title("Accuracy Curves - BiLSTM")
plt.plot(np.arange(1,21,1),train_accuracy, label="Train Accuracy")
plt.plot(np.arange(1,21,1),val_accuracy, label="Val Accuracy")
plt.xticks(np.arange(1,21,1))
plt.yticks(np.arange(0.3,0.9,0.1))
plt.xlabel("# of Epochs")
plt.ylabel("Classification Accuracy (%)")
plt.grid()
plt.legend(loc="lower left")

#Plot Loss Curves
plt.title("Loss Curves - BiLSTM")
plt.plot(np.arange(1,21,1),train_loss, label="Train Loss")
plt.plot(np.arange(1,21,1),val_loss, label="Val Loss")
plt.xticks(np.arange(1,21,1))
plt.yticks(np.arange(0.0,1.5,0.1))
plt.xlabel("# of Epochs")
plt.ylabel("CELoss")
plt.grid()
plt.legend(loc="lower left")